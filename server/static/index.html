<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice Live Agent Demo</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      min-height: 100vh;
      color: #e8e8e8;
      text-align: center;
      padding: 1rem;
      box-sizing: border-box;
    }

    h1 {
      font-size: 2.2rem;
      margin-bottom: 0.8rem;
      color: #ffffff;
      text-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
      font-weight: 600;
    }

    .subtitle {
      font-size: 1.1rem;
      color: #b8c5d6;
      margin-bottom: 2.5rem;
      max-width: 400px;
      line-height: 1.5;
    }

    /* Voice Animation Styles */
    .voice-animation-container {
      position: relative;
      width: 300px;
      height: 120px;
      margin: 2rem auto;
      display: flex;
      align-items: center;
      justify-content: center;
      background: rgba(255, 255, 255, 0.05);
      border-radius: 1rem;
      backdrop-filter: blur(10px);
    }

    .audio-bars {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 8px;
      height: 100%;
    }

    .bar {
      width: 8px;
      min-height: 20px;
      background: rgba(255, 255, 255, 0.2);
      border-radius: 4px;
      transition: all 0.15s ease;
      transform-origin: center;
    }

    .audio-bars.listening .bar {
      background: linear-gradient(180deg, #00d9ff 0%, #0099cc 100%);
      box-shadow: 0 0 10px rgba(0, 217, 255, 0.4);
    }

    .audio-bars.speaking .bar {
      background: linear-gradient(180deg, #ff6b9d 0%, #c44569 100%);
      box-shadow: 0 0 10px rgba(255, 107, 157, 0.4);
    }

    .controls {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      justify-content: center;
      align-items: center;
    }

    button {
      padding: 1rem 2rem;
      font-size: 1.05rem;
      border: none;
      border-radius: 0.75rem;
      cursor: pointer;
      transition: all 0.3s ease;
      font-weight: 500;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    }

    button:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
    }

    button:active:not(:disabled) {
      transform: translateY(0);
    }

    #startBtn {
      background: linear-gradient(135deg, #00b894 0%, #00a381 100%);
      color: white;
    }

    #startBtn:disabled {
      background: linear-gradient(135deg, #4a7c71 0%, #3d6b62 100%);
      cursor: not-allowed;
      opacity: 0.7;
    }

    #startBtn:hover:not(:disabled) {
      background: linear-gradient(135deg, #00d9a7 0%, #00b894 100%);
    }

    #stopBtn {
      background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
      color: white;
    }

    #stopBtn:disabled {
      background: linear-gradient(135deg, #8b5a55 0%, #7a4a45 100%);
      cursor: not-allowed;
      opacity: 0.7;
    }

    #stopBtn:hover:not(:disabled) {
      background: linear-gradient(135deg, #ff5f4f 0%, #e74c3c 100%);
    }

    #toggleTranscriptBtn {
      background: linear-gradient(135deg, #6c5ce7 0%, #5b4cdb 100%);
      color: white;
    }

    #toggleTranscriptBtn:hover {
      background: linear-gradient(135deg, #7d6ff0 0%, #6c5ce7 100%);
    }

    audio {
      margin-top: 2rem;
      display: block;
    }

    .conversation-container {
      width: 90%;
      max-width: 600px;
      margin-bottom: 2rem;
    }

    .conversation-container.hidden {
      display: none;
    }

    .conversation-box {
      background: rgba(255, 255, 255, 0.95);
      border: none;
      border-radius: 1rem;
      padding: 1.25rem;
      height: 300px;
      overflow-y: auto;
      text-align: left;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      font-size: 0.95rem;
      line-height: 1.6;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
      color: #2d3436;
    }

    .conversation-box:empty::before {
      content: "Conversation will appear here...";
      color: #636e72;
      font-style: italic;
    }

    .message {
      margin-bottom: 1rem;
      padding: 0.75rem 1rem;
      border-radius: 0.5rem;
      animation: fadeIn 0.3s ease;
    }

    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(5px); }
      to { opacity: 1; transform: translateY(0); }
    }

    .user-message {
      background-color: #e8f5f3;
      border-left: 4px solid #00b894;
    }

    .assistant-message {
      background-color: #f5e8f0;
      border-left: 4px solid #c44569;
    }

    .message-label {
      font-weight: 600;
      margin-bottom: 0.3rem;
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .user-message .message-label {
      color: #00a381;
    }

    .assistant-message .message-label {
      color: #c44569;
    }

    footer {
      position: fixed;
      bottom: 1rem;
      font-size: 0.85rem;
      color: rgba(255, 255, 255, 0.5);
    }
  </style>
</head>

<body>
  <h1>üéôÔ∏è Voice Live Demo UX</h1>
  <div class="subtitle">Talk to an AI agent in real time. Click below to start.</div>

  <!-- Voice Animation -->
  <div class="voice-animation-container" id="voiceAnimation" style="display: none;">
    <div class="audio-bars" id="audioBars">
      <div class="bar" data-bar="0"></div>
      <div class="bar" data-bar="1"></div>
      <div class="bar" data-bar="2"></div>
      <div class="bar" data-bar="3"></div>
      <div class="bar" data-bar="4"></div>
      <div class="bar" data-bar="5"></div>
      <div class="bar" data-bar="6"></div>
      <div class="bar" data-bar="7"></div>
      <div class="bar" data-bar="8"></div>
      <div class="bar" data-bar="9"></div>
      <div class="bar" data-bar="10"></div>
      <div class="bar" data-bar="11"></div>
    </div>
  </div>

  <div class="conversation-container hidden" id="conversationContainer">
    <div id="conversationBox" class="conversation-box"></div>
  </div>

  <div class="controls">
    <button id="startBtn" onclick="startStreaming()" title="Start talking to the voice agent">
      Start Talking to Agent
    </button>
    <button id="stopBtn" onclick="stopStreaming()" disabled title="End the conversation with the agent">
      Stop Conversation
    </button>
    <button id="toggleTranscriptBtn" onclick="toggleConversationBox()" title="Toggle transcript visibility">
      Show Transcript
    </button>
  </div>

  <audio id="ttsPlayer" autoplay></audio>

  <footer>
    Powered by Azure Voice Live ‚Ä¢ Demo Mode
  </footer>

  <script>
    let mediaStream, source, processor, socket;
    let audioContext = new AudioContext({ sampleRate: 24000 });
    let workletNode;

    // Audio visualization variables
    let analyserNode = null;
    let speakerAnalyserNode = null;
    let dataArray = null;
    let speakerDataArray = null;
    let animationFrame = null;
    let isListening = false;
    let isSpeaking = false;

    function clearConversation() {
      document.getElementById('conversationBox').innerHTML = '';
    }

    function addMessage(text, isUser) {
      const conversationBox = document.getElementById('conversationBox');
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${isUser ? 'user-message' : 'assistant-message'}`;

      const labelDiv = document.createElement('div');
      labelDiv.className = 'message-label';
      labelDiv.textContent = isUser ? 'User:' : 'Assistant:';

      const textDiv = document.createElement('div');
      textDiv.textContent = text;

      messageDiv.appendChild(labelDiv);
      messageDiv.appendChild(textDiv);
      conversationBox.appendChild(messageDiv);

      // Auto-scroll to bottom
      conversationBox.scrollTop = conversationBox.scrollHeight;
    }

    // Load the AudioWorkletProcessor
    async function loadAudioProcessor() {
      await audioContext.audioWorklet.addModule('/static/audio-processor.js');
      workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

      // Setup analyser for speaker output
      speakerAnalyserNode = audioContext.createAnalyser();
      speakerAnalyserNode.fftSize = 256;
      const bufferLength = speakerAnalyserNode.frequencyBinCount;
      speakerDataArray = new Uint8Array(bufferLength);

      // Connect worklet to analyser and then to destination
      workletNode.connect(speakerAnalyserNode);
      speakerAnalyserNode.connect(audioContext.destination);
    }

    async function playAudio(arrayBuffer) {
      if (audioContext.state === 'suspended') await audioContext.resume();
      const int16 = new Int16Array(arrayBuffer);
      const float32 = new Float32Array(int16.length);
      for (let i = 0; i < int16.length; i++) {
        float32[i] = int16[i] / (int16[i] < 0 ? 0x8000 : 0x7FFF);
      }
      workletNode.port.postMessage({ pcm: float32 });
    }

    function stopPlayback() {
      if (workletNode) workletNode.port.postMessage({ clear: true });
    }

    // Setup audio analyser for visualization
    function setupAudioAnalyser() {
      if (audioContext && source) {
        analyserNode = audioContext.createAnalyser();
        analyserNode.fftSize = 256;
        const bufferLength = analyserNode.frequencyBinCount;
        dataArray = new Uint8Array(bufferLength);

        // Connect microphone to analyser
        source.connect(analyserNode);
      }
    }

    // Calculate volume and update bars
    function updateVolumeCircle() {
      let frequencyData = null;

      // When speaking (assistant), use speaker analyser
      if (isSpeaking && speakerAnalyserNode && speakerDataArray) {
        speakerAnalyserNode.getByteFrequencyData(speakerDataArray);
        frequencyData = speakerDataArray;
      }
      // When listening (user), use microphone analyser
      else if (isListening && analyserNode && dataArray) {
        analyserNode.getByteFrequencyData(dataArray);
        frequencyData = dataArray;
      }

      // Update bars based on frequency data
      const audioBars = document.getElementById('audioBars');
      if (audioBars && frequencyData && (isListening || isSpeaking)) {
        const bars = audioBars.querySelectorAll('.bar');
        const barCount = bars.length;
        const dataPerBar = Math.floor(frequencyData.length / (barCount / 2));

        bars.forEach((bar, index) => {
          // Create symmetrical pattern - map bars from center outward
          const centerIndex = (barCount - 1) / 2;
          const distanceFromCenter = Math.abs(index - centerIndex);
          
          // Use lower frequencies for center bars, higher for edges
          const freqIndex = Math.floor(distanceFromCenter * dataPerBar);
          const start = freqIndex;
          const end = Math.min(start + dataPerBar, frequencyData.length);
          
          let sum = 0;
          for (let i = start; i < end; i++) {
            sum += frequencyData[i];
          }
          const average = sum / (end - start);

          // Calculate bar height based on frequency (20px to 100px)
          const minHeight = 20;
          const maxHeight = 100;
          const height = minHeight + (average / 255) * (maxHeight - minHeight);
          bar.style.height = `${height}px`;
        });

        // Update class based on state
        audioBars.className = 'audio-bars ' + (isSpeaking ? 'speaking' : 'listening');
      }

      // Continue animation
      if (isListening || isSpeaking) {
        animationFrame = requestAnimationFrame(updateVolumeCircle);
      }
    }

    // Start volume animation
    function startVolumeAnimation(listening = true) {
      isListening = listening;
      isSpeaking = !listening;

      const voiceAnimation = document.getElementById('voiceAnimation');
      if (voiceAnimation) {
        voiceAnimation.style.display = 'flex';
      }

      if (animationFrame) {
        cancelAnimationFrame(animationFrame);
      }
      updateVolumeCircle();
    }

    // Stop volume animation
    function stopVolumeAnimation() {
      isListening = false;
      isSpeaking = false;

      if (animationFrame) {
        cancelAnimationFrame(animationFrame);
        animationFrame = null;
      }

      const voiceAnimation = document.getElementById('voiceAnimation');
      if (voiceAnimation) {
        voiceAnimation.style.display = 'none';
      }

      const audioBars = document.getElementById('audioBars');
      if (audioBars) {
        // Reset all bars to minimum height
        const bars = audioBars.querySelectorAll('.bar');
        bars.forEach(bar => {
          bar.style.height = '20px';
        });
        audioBars.className = 'audio-bars';
      }
    }

    function float32ToInt16(float32Array) {
      const int16 = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return int16;
    }

    async function startMicrophone() {
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      source = audioContext.createMediaStreamSource(mediaStream);
      processor = audioContext.createScriptProcessor(4096, 1, 1);

      // Setup analyser for visualization
      setupAudioAnalyser();

      processor.onaudioprocess = (event) => {
        const input = event.inputBuffer.getChannelData(0);
        const pcm = float32ToInt16(input);
        if (socket?.readyState === WebSocket.OPEN) {
          socket.send(pcm.buffer);
        }
      };

      // ONLY route mic input into the processor
      source.connect(processor);
      processor.connect(audioContext.destination); // destination is silent when context's output not used

      // Start listening animation
      startVolumeAnimation(true);
    }

    function stopMicrophone() {
      if (analyserNode) {
        analyserNode.disconnect();
        analyserNode = null;
      }
      if (processor && typeof processor.disconnect === "function") {
        processor.disconnect();
        processor = null;
      }
      if (mediaStream && mediaStream.getTracks) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      stopVolumeAnimation();
    }

    function stopStreaming() {
      stopPlayback(); // Stop any audio that's currently playing
      stopVolumeAnimation(); // Stop animation

      // Immediately update button states for better UX
      document.getElementById("startBtn").disabled = false;
      document.getElementById("stopBtn").disabled = true;

      if (socket) {
        socket.close();
      }
    }

    function startStreaming() {
      clearConversation(); // Clear conversation when starting
      let wsProtocol = window.location.protocol === "https:" ? "wss" : "ws";
      let wsHost = window.location.host;
      socket = new WebSocket(`${wsProtocol}://${wsHost}/web/ws`);
      socket.binaryType = "arraybuffer";

      socket.onopen = async () => {
        console.log("WebSocket opened");
        try {
          await audioContext.resume();
          await startMicrophone();
          document.getElementById("startBtn").disabled = true;
          document.getElementById("stopBtn").disabled = false;
        } catch (error) {
          console.error("Failed to start microphone:", error);
          // Reset button states on error
          document.getElementById("startBtn").disabled = false;
          document.getElementById("stopBtn").disabled = true;
          socket.close();
        }
      };

      socket.onmessage = async (event) => {
        if (typeof event.data === "string") {
          const msg = JSON.parse(event.data);
          if (msg.Kind === "StopAudio") {
            console.log("Audio stopped")
            stopPlayback();
            startVolumeAnimation(true); // Back to listening mode
          }
          if (msg.Kind === "UserTranscription") {
            console.log("User transcript:", msg.Text);
            addMessage(msg.Text, true); // true for user message
          }
          if (msg.Kind === "AssistantTranscription") {
            console.log("Assistant transcript:", msg.Text);
            addMessage(msg.Text, false); // false for assistant message
          }
        }
        else if (event.data instanceof ArrayBuffer) {
          try {
            startVolumeAnimation(false); // Switch to speaking mode
            playAudio(event.data);
          } catch (e) {
            console.error("Failed to decode audio:", e);
          }
        }
        else {
          console.log("unknown message", event.data);
        }
      }

      socket.onclose = () => {
        console.log("WebSocket closed");
        stopMicrophone();
        document.getElementById("startBtn").disabled = false;
        document.getElementById("stopBtn").disabled = true;
      };

      socket.onerror = (err) => {
        console.error("WebSocket error", err);
        // Ensure buttons are in correct state on error
        document.getElementById("startBtn").disabled = false;
        document.getElementById("stopBtn").disabled = true;
      };
    }

    function toggleConversationBox() {
      const conversationContainer = document.getElementById('conversationContainer');
      const toggleBtn = document.getElementById('toggleTranscriptBtn');

      if (conversationContainer.classList.contains('hidden')) {
        conversationContainer.classList.remove('hidden');
        toggleBtn.textContent = 'Hide Transcript';
      } else {
        conversationContainer.classList.add('hidden');
        toggleBtn.textContent = 'Show Transcript';
      }
    }

    // Initialize the audio processor
    loadAudioProcessor();
  </script>
</body>

</html>